\section{Background and Motivation}
\label{s:background}
This section provides background on classical access patterns~\cite{}~(\autoref{ss:access-pattern-based-analysis}) and on how prior work designs algorithms to cover them, both non-adaptively~\cite{} and adaptively~\cite{}~(\autoref{ss:non-adaptive-and-adaptive-algorithms}). However, the real-world access is much complex than ideal access patterns. We observe that most eviction algorithms operate in a metastable state, leading to a significant decrease in hit ratio for some benchmarks and configurations~(\autoref{ss:the-metastable-phenomenon}).

\subsection{Access pattern-based analysis}
\label{ss:access-pattern-based-analysis}
Many work analyzes access streams based on metrics to classify them into specific access patterns~\cite{}. 
%
The locality of accesses is an intrinsic property of workloads, which reveals the potential for caching.
%
The temporal locality focuses on the time interval between two accesses to the same object, which is typically measured by the reuse distance~\cite{}.
%
The spatial locality concentrates on the correlation between accesses to different objects, (\eg, adjacent accesses and adjacent addresses)~\cite{}.
%
A frequency-based analysis checks whether the access frequency distribution follows a specific distribution, (\eg, Zipfian distribution)~\cite{}.
%
On the other hand, some work againsts inference accesses, which deviates from the typical access patterns~\cite{}.
%
To deal with real-world behaviors, some work analyzes the daily or long time periodicity of accesses~\cite{}, but this paper discusses the logical access only.
%
Based on the above metrics, there are four typical primitive access patterns: \textit{LFU-friendly}, \textit{LRU-friendly}, \textit{Churn}, and \textit{Scan}~\cite{}.

\textbf{An LFU-friendly pattern} has a skewed access frequency distribution, where a small portion of objects are accessed frequently, and most objects are accessed infrequently.
%
The \textit{LFU} eviction algorithm is the best choice for this pattern, which keeps the frequently accessed objects in the cache and evicts the infrequently accessed ones.

\textbf{An LRU-friendly pattern} has good temporal locality with a small reuse distance, where the recently accessed objects are likely to be accessed again soon.
%
The \textit{LRU} eviction algorithm is the best choice for this pattern, which keeps the recently accessed objects in the cache and evicts the least recently used ones.

\textbf{A \XXX{Churn} pattern} is a set of repeated accesses, where all objects are accessed with equal probability~\cite{}.

\textbf{A Scan pattern} is a set of accesses where all objects are accessed only once, which means it is an interference with no reuse.

However, prior work cannot separate a workload into the above four primitive access patterns perfectly.
%
Most workloads are a mixture of the above primitive access patterns, and the mixture is complex.
%
Some of them are a combination of two or more primitive access patterns, and some of them are not similar to any primitive access patterns.

\subsection{Non-adaptive and adaptive algorithms}
\label{ss:non-adaptive-and-adaptive-algorithms}
Guided by access patterns, an algorithm either covers them statically or adapts to them. 
%
An adaptive algorithm will adjust the operation or cache size for data in different patterns, whereas a non-adaptive algorithm consistently uses a fixed criterion and workflow. 
%
However, adaptive algorithms are not the best choice for various patterns, and a non-adaptive algorithm, S3FIFO, outperforms them in many workloads.

Non-adaptive algorithms filter a major access pattern from interference patterns.
%
Deviated from the LRU algorithm, SLRU~\cite{} changes the insertion point to filter out the \textit{Scan} pattern from the \textit{LRU-friendly} pattern.
%
As an algorithm that focuses more on access frequency, WTinyLFU~\cite{} uses a Count-Min Sketch~\cite{} to record long-term access counts for \textit{LFU-friendly} pattern.
%
2Q~\cite{} separates the first access and subsequent accesses to filter out the \textit{Scan} pattern, and S3FIFO~\cite{} leverage three FIFO queues to quickly demote \textit{Scan} pattern.
%
Hyperbolic~\cite{}, LHD~\cite{}, and GDSF~\cite{} consider both access frequency and recency with a new metric to cover both \textit{LFU-friendly} and \textit{LRU-friendly} patterns.
%
With the fixed criterion metric and workflow, these non-adaptive algorithms are reliable in specific access patterns.

Adaptive algorithms adjust the subpart size for patterns or the criterion metric for spatial or temporal adaptive. 
%
Spatial adaptive algorithms, ARC~\cite{} and CAR~\cite{}, separate data into recency and frequency parts, which means \textit{LRU-friendly} and \textit{LFU-friendly} patterns.
%
Then, they use two ghost caches with only metadata for the marginal hit ratios to guide the adjustment of the two parts.
%
Temporal adaptive algorithms, LeCaR~\cite{} and Cacheus~\cite{}, propose a framework, which leverages two basic policies and invokes them with different probabilities.
%
LeCaR uses LRU and LFU as two basic policies, and Cacheus uses LRU-Scan-Resistant and LFU-Churn-Resistant as two basic policies.
%
However, adaptive algorithms cannot separate primitive access patterns perfectly in either spatial or temporal dimensions, which incurs a great overhead in the framework.

\subsection{The metastable phenomenon}
\label{ss:the-metastable-phenomenon}


This section presents a background on software cache systems~(\autoref{ss:background-software-cache}) and a phenomenon that most current replacement policies operate in a metastable state~(\autoref{ss:performance-policy-change}).
%
We analyze the root causes of this issue from two perspectives: basic access pattern classification~(\autoref{ss:access-patterns}) and replacement policy design~(\autoref{ss:adaption-criteria}).

\XXX{Captalize the subsection title properly. Please check prior papers}
\XXX{Certain sentences need a citations}

\subsection{Background on software cache systems}
\label{ss:background-software-cache}
\XXX{Do we need this subsection? Is there a risk that reviewers do not know what is a cache?}
Software cache systems are widely \XXX{implemented} \DZ{Avoid the term implement as much as possible in a systems paper. Implement means trivial} in various fields to improve service performance.
%
In storage systems, faster devices are typically more expensive and have smaller capacities. 
%
To reduce costs and minimize data access latency, \XXX{we} \DZ{Who? This sounds you implement cache.} place a subset of the data on high-speed devices to form a cache system.
%
\XXX{In the KV-store, CDN, and memory devices, the cache stores Key-Value pairs, web objects, and data blocks, respectively, and we use the term \textit{object} to represent the data unit stored in the cache} \DZ{Citation?}.
%

Generally, \XXX{cache systems have three main research directions: \cc{access pattern analysis}, \cc{cache replacement policies}, and \cc{cache prefetching}.} 
%
\DZ{??} 
%
Access pattern analysis helps design replacement policies or adjust parameters in real-time to adapt to different situations.
%
Cache replacement policies determine which object to retain or evict; a higher hit ratio indicates better performance. 
%
When data access patterns are predictable, prefetching can anticipate future accesses and load data into the cache. 
%
For ease of deployment, cache systems typically use a specific replacement policy to handle the current workload, decoupling prefetching from the replacement strategy.

\subsection{The metastable policies}
\label{ss:performance-policy-change}
Current replacement policies are in a metastable state, which means that if data access patterns or cache size change, the overall performance will degrade accordingly.
%
In production environments, the policy performs poorly under specific characteristics, and increasing the cache size can sometimes lead to negative returns.
%
We evaluate 16 policies in \XXX{5000+} workloads with different cache sizes and calculate the relative hit ratio to the best policy in the policy set.
%
All policies achieve the best performance in a specific cache size and suffer from a tail performance about 20\% to 60\% relative hit ratio to the best policy.
%

%If we use a specific replacement policy for all traces in the benchmarks, there are \XXX{XX}\% of traces that will lose \XXX{XX}\% performance.
%
%What's more, even with the same trace, if we slightly change the cache size without adjusting the replacement policy, more than 5\% of the performance potential may remain unexploited.


Prior work cannot fit various \textbf{access patterns} in a benchmark.
%
As shown in figure ~\autoref{fig:cloudphysics}(a), we evaluate the performance of 16 cache replacement policies~\cite{} in Cloudphysics~\cite{}. 
%
CloudPhysics provides \XXX{106} traces. 
%
We evaluate the hit ratio of replacement policies under a given cache size, which is relative to the working set size(WSS). 
%
For each workload, we select the replacement policy with the highest hit ratio as the optimal policy. 
%
Then, we calculate each policy's performance relative to the optimal policy and compute the geometric mean across all traces.
%
We observe that S3FIFO~\cite{} performs better when the cache size is small, while LIRS performs better when the cache size is large. 
%
Most replacement policies achieve an average performance of under 95\%, which means that using the same replacement policy for all traces would result in sacrificing nearly 5\% of potential performance gains.
%
As shown in figure ~\autoref{fig:cloudphysics}(b), the worst-performing 10\% of workloads for both S3FIFO and ARC only achieve 80\% performance when the cache size is 3\% WSS.
%
While \sys is the best policy in all cache sizes, it is the \textbf{best} policy in \textbf{50\%} percent of workloads, and achieves \textbf{95\%} performance in \textbf{95\%} of traces.
%


\DZ{I am lost. How is what you measure in this paragraph different from the last paragrap. }
For the same workload, the performance of different policies varies significantly with different \textbf{cache sizes}.
%
As shown in figure~\autoref{fig:mrcdemo}(a), ARC~\cite{} and LIRS~\cite{} achieve the best performance in different cache sizes in systor~\cite{}. 
%
We select the replacement policy with the highest hit ratio as the optimal policy and compute the relative hit ratio of each policy to the optimal policy.
%
In figure~\autoref{fig:mrcdemo}(b), when the cache size enlarges from 80GB to 140GB, ARC closely follows the optimal policy, but when we enlarge the cache size to 1600GB, the performance of ARC degrades by about 20\%.
%
What is more, the performance of WTinyLFU~\cite{} is metastable at all cache sizes.
%
When the cache size is 50GB, it achieves 99.3\% hit ratio of the best policy, but when the cache size is 40BG or 60GB, it degrades to 88.7\% and 90.0\% respectively.
%
When we consider \sys, it is the best policy and outperforms other policies by about 2\%.

\subsection{Adaption for mixed access patterns}
\label{ss:adaption-criteria}
Prior work splits the cache into different parts to handle access patterns or filter interference between them.
%
They commonly split the cache into three functional parts: \cc{the main cache}, \cc{the filter cache}, and \cc{the ghost cache}.
%
If the access patterns are a major pattern and an interference pattern, the main cache stores the major pattern, and the filter cache filters the interference pattern.
%
If there are two major patterns, they split the main cache into two parts, one for each major pattern. 
%
Many policies use the ghost cache to store the metadata of the evicted objects~\cite{}, \XXX{which can be used to adjust the size of other parts.}
%   
\DZ{Lost, what do you mean here?}Â·
%The missing records in the ghost cache can help adjust the size of the main cache and filter cache.
The access patterns in~(\S\ref{ss:access-patterns}) and incomplete criteria for adjustment mechanisms limit the capability of the replacement policy.

%They change the part size according to the hit information and tag the object type with static criteria.

\textbf{The major pattern and interference pattern.} In order to filter scan in the LRU-friendly pattern, SLRU~\cite{} adds the new object to the filter cache and promotes it to the main cache if it is reaccessed.
%
LIRS~\cite{} uses the ghost cache to identify the recently accessed objects and promotes them to the main cache if they are reaccessed.
%
In order to filter period repeated access in the LFU-friendly pattern, MQ~\cite{} splits the cache into many parts to store the objects with different access frequencies and evicts the object with the lowest access frequency.
%
TinyLFU~\cite{} uses the filter part to record the access frequency, and only those with high access frequency are admitted into the main cache.
%
S3-FIFO~\cite{} analyzes the access pattern and finds that most benchmarks consist of many scan accesses (named One-hit-wonder), so it uses a small filter cache to decrease newly accessed objects and a ghost cache to pick up the wrongly evicted objects.
% 

\textbf{Dueling for two major patterns.} Others think that the replacement policy should adapt to the access pattern, so they design the policy to cover both LRU-friendly and LFU-friendly patterns.
%
LeCaR~\cite{} and Cacheus~\cite{} leverage two major replacement policies (e.g., LRU and LFU) and adjust the probability of applying each policy based on the hit information collected from the ghost cache.
%
2Q~\cite{}, ARC~\cite{} and CAR~\cite{} consider that the first access to objects and subsequent accesses exhibit different patterns, and therefore 
\DZ{How is us relevant to 2Q, ARC, and CAR?}
\XXX{we} need to manage them separately. 
%
They use ghost caches to adjust the proportion of different types of objects accordingly.
%Dueling for LRU-friendly and LFU-friendly. ARC, CAR, 2Q,LeCaR,Cacheus,LRFU

\textbf{The adaptation process in prior work is short-sighted.}
%
These policies based on basic access patterns are insufficient and suffer from severe performance degradation.% different access pattern. 
%
First, basic access patterns are unaware of the cache size, causing the replacement policy's performance to fluctuate significantly as the cache size changes. 
%
As shown in figure~\ref{fig:mrcdemo}(b) and figure~\ref{fig:alibb}(a), the performance of ARC, WTinyLFU, and LeCaR degrade when the cache size is larger.
%
Second, they are too positive about the objects in the filter cache and ghost cache, leading to misjudgment about the overall access behavior. 
%relying solely on the objects in the ghost cache may lead to misjudgments about the overall access behavior.
%
If a request hits the ghost cache, these policies increase the cache size of the corresponding pattern or the usage probability of the competing pattern as compensation.
%
What's more, these policies do not account for hits in the main cache, in ARC and CAR, many objects that are accessed only twice consecutively can cause cache pollution by evicting important objects from the main cache.
%
We will discuss it in (\S\ref{ss:dynamic-adjust}). 

\textbf{Summary:We need to take the overall cache access behavior into account to adapt to mixed access patterns.}


%
%For trace $\text{AAAABBCCCDDCCA}$, if the cache size is 2, the base policy is LRU, so it is LRU-friendly, but if the cache size is 3, the base policy is LFU, so it is LFU-friendly.
%