\section{Background and Motivation}
\label{s:background}
This section presents a background on software cache systems~(\autoref{ss:background-software-cache}) and a phenomenon that most current replacement policies operate in a metastable state~(\autoref{ss:performance-policy-change}).
%
We analyze the root causes of this issue from two perspectives: basic access pattern classification~(\autoref{ss:access-patterns}) and replacement policy design~(\autoref{ss:adaption-criteria}).

\XXX{Captalize the subsection title properly. Please check prior papers}
\XXX{Certain sentences need a citations}

\subsection{Background on software cache systems}
\label{ss:background-software-cache}
\XXX{Do we need this subsection? Is there a risk that reviewers do not know what is a cache?}
Software cache systems are widely \XXX{implemented} \DZ{Avoid the term implement as much as possible in a systems paper. Implement means trivial} in various fields to improve service performance.
%
In storage systems, faster devices are typically more expensive and have smaller capacities. 
%
To reduce costs and minimize data access latency, \XXX{we} \DZ{Who? This sounds you implement cache.} place a subset of the data on high-speed devices to form a cache system.
%
\XXX{In the KV-store, CDN, and memory devices, the cache stores Key-Value pairs, web objects, and data blocks, respectively, and we use the term \textit{object} to represent the data unit stored in the cache} \DZ{Citation?}.
%

Generally, \XXX{cache systems have three main research directions: \cc{access pattern analysis}, \cc{cache replacement policies}, and \cc{cache prefetching}.} 
%
\DZ{??} 
%
Access pattern analysis helps design replacement policies or adjust parameters in real-time to adapt to different situations.
%
Cache replacement policies determine which object to retain or evict; a higher hit ratio indicates better performance. 
%
When data access patterns are predictable, prefetching can anticipate future accesses and load data into the cache. 
%
For ease of deployment, cache systems typically use a specific replacement policy to handle the current workload, decoupling prefetching from the replacement strategy.

\subsection{The metastable policies}
\label{ss:performance-policy-change}
Current replacement policies are in a metastable state, which means that if data access patterns or cache size change, the overall performance will degrade accordingly.
%
In production environments, the policy performs poorly under specific characteristics, and increasing the cache size can sometimes lead to negative returns.
%
We evaluate 16 policies in \XXX{5000+} workloads with different cache sizes and calculate the relative hit ratio to the best policy in the policy set.
%
All policies achieve the best performance in a specific cache size and suffer from a tail performance about 20\% to 60\% relative hit ratio to the best policy.
%

%If we use a specific replacement policy for all traces in the benchmarks, there are \XXX{XX}\% of traces that will lose \XXX{XX}\% performance.
%
%What's more, even with the same trace, if we slightly change the cache size without adjusting the replacement policy, more than 5\% of the performance potential may remain unexploited.


Prior work cannot fit various \textbf{access patterns} in a benchmark.
%
As shown in figure ~\autoref{fig:cloudphysics}(a), we evaluate the performance of 16 cache replacement policies~\cite{} in Cloudphysics~\cite{}. 
%
CloudPhysics provides \XXX{106} traces. 
%
We evaluate the hit ratio of replacement policies under a given cache size, which is relative to the working set size(WSS). 
%
For each workload, we select the replacement policy with the highest hit ratio as the optimal policy. 
%
Then, we calculate each policy's performance relative to the optimal policy and compute the geometric mean across all traces.
%
We observe that S3FIFO~\cite{} performs better when the cache size is small, while LIRS performs better when the cache size is large. 
%
Most replacement policies achieve an average performance of under 95\%, which means that using the same replacement policy for all traces would result in sacrificing nearly 5\% of potential performance gains.
%
As shown in figure ~\autoref{fig:cloudphysics}(b), the worst-performing 10\% of workloads for both S3FIFO and ARC only achieve 80\% performance when the cache size is 3\% WSS.
%
While \sys is the best policy in all cache sizes, it is the \textbf{best} policy in \textbf{50\%} percent of workloads, and achieves \textbf{95\%} performance in \textbf{95\%} of traces.
%


\DZ{I am lost. How is what you measure in this paragraph different from the last paragrap. }
For the same workload, the performance of different policies varies significantly with different \textbf{cache sizes}.
%
As shown in figure~\autoref{fig:mrcdemo}(a), ARC~\cite{} and LIRS~\cite{} achieve the best performance in different cache sizes in systor~\cite{}. 
%
We select the replacement policy with the highest hit ratio as the optimal policy and compute the relative hit ratio of each policy to the optimal policy.
%
In figure~\autoref{fig:mrcdemo}(b), when the cache size enlarges from 80GB to 140GB, ARC closely follows the optimal policy, but when we enlarge the cache size to 1600GB, the performance of ARC degrades by about 20\%.
%
What is more, the performance of WTinyLFU~\cite{} is metastable at all cache sizes.
%
When the cache size is 50GB, it achieves 99.3\% hit ratio of the best policy, but when the cache size is 40BG or 60GB, it degrades to 88.7\% and 90.0\% respectively.
%
When we consider \sys, it is the best policy and outperforms other policies by about 2\%.


\subsection{Access patterns}
\label{ss:access-patterns}
Prior work classifies the access patterns into four basic types and a mixture of the basic types. 
%
This classification method is very intuitive, and we determine the trace type after the analysis (e.g., implementing the replacement policy).

\textbf{LRU-friendly} is a set of access that the \cc{least recently used} (LRU) replacement policy is the best choice for the workload.

\textbf{LFU-friendly} is a set of access that the \cc{least frequently used} (LFU) replacement policy is the best choice for the workload.

\textbf{Scan} is a set of access that all objects are accessed only once, which means no replacement policy can deal with it.

\textbf{Period repeated access} is a set of repeated accesses, and a more restrictive definition is that the objects are accessed with equal probability~\cite{}(cacheus), which means the hit ratio is related to the cache size. If the cache size is larger than the repeated objects, the hit ratio is 100\%, and if the cache size is smaller than the repeated objects, the hit ratio is a fixed number.

%\textbf{Mixture of the basic types} \dots
%This classification method is neither object-based nor access sequence-based, but only for the trace type.
\textbf{This classification is not a cache size-aware method.}
%
It cannot tell us the object type, only the trace type.
%
When the cache size changes, the trace type will change.
%so defining a mixture of the basic types is confusing. 
%
If an object is accessed many times in a short sequence, without access for a long sequence, and then reaccessed, we cannot determine whether it is LRU-friendly or LFU-friendly. 
%
If the footprint of the long access sequence is smaller than the cache size, is the object both LRU-friendly and LFU-friendly? 
%
This classification method is retrospective — the first access to each object is treated as part of a scan, and only after the second access can the object be classified as another type.
%
For trace $\text{AAAABBCCCDDCCA}$, if the cache size is 2, the base policy is LRU, so it is LRU-friendly, but if the cache size is 3, the base policy is LFU, so it is LFU-friendly.
%

\textbf{Summary: We need a method to classify the object type with cache size awareness.}


\subsection{Adaption for mixed access patterns}
\label{ss:adaption-criteria}
Prior work splits the cache into different parts to handle access patterns or filter interference between them.
%
They commonly split the cache into three functional parts: \cc{the main cache}, \cc{the filter cache}, and \cc{the ghost cache}.
%
If the access patterns are a major pattern and an interference pattern, the main cache stores the major pattern, and the filter cache filters the interference pattern.
%
If there are two major patterns, they split the main cache into two parts, one for each major pattern. 
%
Many policies use the ghost cache to store the metadata of the evicted objects~\cite{}, \XXX{which can be used to adjust the size of other parts.}
%   
\DZ{Lost, what do you mean here?}·
%The missing records in the ghost cache can help adjust the size of the main cache and filter cache.
The access patterns in~(\S\ref{ss:access-patterns}) and incomplete criteria for adjustment mechanisms limit the capability of the replacement policy.

%They change the part size according to the hit information and tag the object type with static criteria.

\textbf{The major pattern and interference pattern.} In order to filter scan in the LRU-friendly pattern, SLRU~\cite{} adds the new object to the filter cache and promotes it to the main cache if it is reaccessed.
%
LIRS~\cite{} uses the ghost cache to identify the recently accessed objects and promotes them to the main cache if they are reaccessed.
%
In order to filter period repeated access in the LFU-friendly pattern, MQ~\cite{} splits the cache into many parts to store the objects with different access frequencies and evicts the object with the lowest access frequency.
%
TinyLFU~\cite{} uses the filter part to record the access frequency, and only those with high access frequency are admitted into the main cache.
%
S3-FIFO~\cite{} analyzes the access pattern and finds that most benchmarks consist of many scan accesses (named One-hit-wonder), so it uses a small filter cache to decrease newly accessed objects and a ghost cache to pick up the wrongly evicted objects.
% 

\textbf{Dueling for two major patterns.} Others think that the replacement policy should adapt to the access pattern, so they design the policy to cover both LRU-friendly and LFU-friendly patterns.
%
LeCaR~\cite{} and Cacheus~\cite{} leverage two major replacement policies (e.g., LRU and LFU) and adjust the probability of applying each policy based on the hit information collected from the ghost cache.
%
2Q~\cite{}, ARC~\cite{} and CAR~\cite{} consider that the first access to objects and subsequent accesses exhibit different patterns, and therefore 
\DZ{How is us relevant to 2Q, ARC, and CAR?}
\XXX{we} need to manage them separately. 
%
They use ghost caches to adjust the proportion of different types of objects accordingly.
%Dueling for LRU-friendly and LFU-friendly. ARC, CAR, 2Q,LeCaR,Cacheus,LRFU

\textbf{The adaptation process in prior work is short-sighted.}
%
These policies based on basic access patterns are insufficient and suffer from severe performance degradation.% different access pattern. 
%
First, basic access patterns are unaware of the cache size, causing the replacement policy's performance to fluctuate significantly as the cache size changes. 
%
As shown in figure~\ref{fig:mrcdemo}(b) and figure~\ref{fig:alibb}(a), the performance of ARC, WTinyLFU, and LeCaR degrade when the cache size is larger.
%
Second, they are too positive about the objects in the filter cache and ghost cache, leading to misjudgment about the overall access behavior. 
%relying solely on the objects in the ghost cache may lead to misjudgments about the overall access behavior.
%
If a request hits the ghost cache, these policies increase the cache size of the corresponding pattern or the usage probability of the competing pattern as compensation.
%
What's more, these policies do not account for hits in the main cache, in ARC and CAR, many objects that are accessed only twice consecutively can cause cache pollution by evicting important objects from the main cache.
%
We will discuss it in (\S\ref{ss:dynamic-adjust}). 

\textbf{Summary:We need to take the overall cache access behavior into account to adapt to mixed access patterns.}


