\clearpage

\section{The \sys Design}
\label{s:design}

Following our approach to identify and classify workload access patterns 
(\autoref{}), this section presents the design of \sys, an efficient cache 
eviction algorithm that adapts to a broad spectrum of access patterns.
%
% This section presents the design goals of \sys (\autoref{ss:design-goals}), an overview of its components and workflow (\autoref{ss:overview}), and a discussion of its limitations (\autoref{ss:discussion-and-limitations}).



\subsection{\sys Components}
\label{ss:components}

\autoref{fig:overview} shows the key components and the workflow of \sys. 
%
We next present the core design of \sys and explain how it meets the design goals~(\S\ref{}). 
%
To facilitate the presentation, we defer the discussion of the performance and 
space optimizations to \autoref{s:impl}. 

%
\DZ{Need to see how we should define metadata}
\DZ{Need to provide a high-level idea of how the algorithm works, especially connect to the access patterns.}
The main part of \sys consists of four FIFO queues: a filter queue~(\ABB{F}), a core queue ~(\ABB{C}), a staging queue~(\ABB{S}), and a ghost queue~(\ABB{G}). 
%
In addition, \sys includes an auxiliary store~(\ABB{AUX}) that stores \XXX{}. 
%
The filter, core, and staging queues record both data and metadata. 
%
The ghost queue and the auxiliary store record only metadata. 

\PN{Task division.}
%
We next explain how \sys divides tasks among components to achieve the overall caching policy (\S\ref{}): screening out cold-rare objects, while retaining the other three types of objects. 

\DZ{Let me know if the term is correct.}
The filter queue is a small queue that quickly \XXX{demotes} likely cold-rare objects, thereby minimizing their residency in the cache.  
%
The core queue consumes most of the cache space to store objects that survive the filter queue. 
%
The ghost queue stores metadata for objects demoted by the filter queue,  serving as a second chance to identify the access patterns of the objects. 
%
This is necessary since the filter queue may falsely filter some objects due to its aggressive, quick demotion policy.
%

The staging queue stores objects 1) evicted from the core queue, and 2) promoted from the ghost queue.
%
Upon object eviction, the staging queue verifies its access 
patterns and reinserts it into the core queue if its access pattern is 
not cold-rare.
%
For objects evicted from the core queue, the staging queue maximizes the 
hit rate by retaining objects with good locality, following the spirit of FIFO-reinsertion~\cite{}.
%
For objects promoted from the ghost queue, the staging queue provides another chance to verify their access patterns before eviction. 


\DZ{Need to explain somewhere that the length of the auxiliary store.}
The auxiliary store operates independently from the aforementioned components. 
%
It records the hotness and popularity of objects to assist \sys in identifying object access patterns. 


\PN{Space allocation.}
%
The filter queue, core queue, and the staging queue use \XXX{}
of the cache place, respectively. 
% 
The ghost queue stores the metadata of one epoch, while the auxiliary store \XXX{}. 
%
We identify these values through sensitivity analysis~(\S\ref{}) and found that they work well across a wide range of real-world workloads. 

\DZ{Key invairant: observe the access of the data for one epoch.}
\subsection{\sys Workflow}
\label{ss:workflow}

\PN{Overview.}
%
As shown in \autoref{}, for each object accessed for the first time, \sys places the object into the filter queue and observes its access pattern with the help of the auxiliary store.
%
Upon eviction from the filter queue, depending on its access pattern, \sys either promotes the object to the core queue (for hot or popular objects), or evicts the object but records its metadata in the ghost queue (offering a second chance to classify its access patterns as discussed in \S\ref{}).

An object that enters the core queue will later be evicted to the staging queue.
%
Upon eviction from the staging queue, \sys reexamines the object's access pattern, and only evicts it if is cold and rare. 
%
Otherwise, \sys reinserts the object into the core queue.

For an object in the ghost queue, upon access, \sys must bring it back into the cache.
%
Specifically, \sys reexamines the object's access pattern and promotes it to the main queue if it is hot or popular.
%
Otherwise, \sys promotes it to the staging queue. 
%
In the latter case, if the object is evicted from the staging queue, \sys places it back in the ghost queue, ensuring that \sys observes the object's access pattern for a full epoch. 

\XXX{The auxiliary store operates}

We next present the detailed algorithm of \sys in \autoref{}. 

\PN{Recording objects' hotness and popularity.}
%
For each incoming object, regardless of whether it results in a cache hit or miss, \ABB{AUX} records \XXX{}

%its hotness. 

\PN{Cache hits.}
%
Upon a cache hit~(\ie, the object resides in one of the \ABB{F}, \ABB{C}, or \ABB{S} queues), \sys simply updates the hotness of the object~(L\XXX{}),  which will later be used to \XXX{}

\PN{Cache misses.}
%
\DZ{Lines 8 and 10 should have comments stating cache hits or miss.}
%
Upon a cache miss (L\XXX{}), \sys first checks if the cache is full, and if so, evicts an object~(L\XXX{}).
%
Next, \sys checks if the object has been previously encountered and resides in the ghost queue~(L\XXX{}).
%
If not, \sys treats the object as a new object and inserts it into the filter queue~(L\XXX{}).
%
Otherwise, \sys checks the object's access pattern~(L\XXX{}-\XXX{}) and inserts it into either the core queue~(L\XXX{}) or the staging queue~(L\XXX{}). 

\PN{Object eviction.}
%
When the cache is full, \sys first tries to evict from the filter queue (L\XXX{}). 
%
If no object can be evicted~(due to \eg, all objects evicted from the filter queue go to the core queue), \sys first evicts objects from the core queue to the staging queue~(L\XXX{}). 
%
Next, \sys evicts objects from the staging queue~(L\XXX{}).
%
Such an eviction guarantees to succeed since \XXX{}. 
%
As discussed earlier, for an evicted object coming from the ghost queue, 
\sys inserts it back~(L\XXX{}-\XXX{}). 




\subsection{Adapting to Various Access Patterns}

\PN{Hotness and Popularity Thresholds.}
%
\autoref{} presents \syss adaptive algorithm. 
%
Departing from prior work that uses \XXX{} to achieve adaptiveness~(\S\ref{}), 
%
\syss adaptiveness is enabled by its use of dynamic thresholds to classify 
an object's access pattern. 
%
An object is hot (and popular, respectively) if its \XXX{} exceeds the hotness (and popularity) threshold~(L\XXX{} in \autoref{}).

\DZ{How does changing the thresholds adapt to different access patterns?}
Intuitively, a higher hotness thresholds results in fewer objects being classified as hot, thereby \XXX{}. 

\PN{Self-tuning the thresholds based on the cache size.}
\XXX{Motivate why doing this.}
%

\XXX{Switch to implementationn from this point now.}
To decide the hotness threshold, \sys maintains in the auxiliary store 
the distribution of hotness counters~(\ie, how many cached objects fall under each specific \XXX{counter} value)~(L\XXX{}). 
%
\XXX{When does it got updated?}
\XXX{How do we decide the popularity threshold?}

\PN{Adapting to different access patterns.}
\XXX{Perhaps list one trace that S3FIFO does not perform well and argue that 
how \sys adapts to it}


\PN{Adaptiveness.}
%
For example, repeated accesses~(Churn) have a good locality, but they will pollute cache if cache cannot hold them all.
%
If cache could hold all of them, they are \textit{frequent-persistent} data and stay in \ABB{M}.
%
If cache cannot hold all of them and \ABB{G} could observe the repeatness, \sys takes part of them as \textit{frequent-ephemeral} data in \ABB{M}, and then \ABB{H} gradually raises the hotness threshold to block other \textit{infrequent-persistent} data.
%
If \ABB{G} cannot observe the repeatness, \sys guesses part of them to \ABB{S} with the help of \ABB{K}.



\section{Realizing \sys}
% 
We implemented \sys in both \cachelib~\cite{}, a development library for caching systems deployed at Facebook, and \libcachesim~\cite{}, an efficient cache simulator. 
%
We choose these two frameworks to benefit from that many eviction algorithms~(\S\ref{}) are already implemented on them.
%
We also contributed to \cachelib an implementation of \arc and \car, and \libcachesim an implementation of \car, respectively.  


\PN{Minimizing the auxiliary store.}
%
A practical implementation of \sys must minimize the space overhead incurred by the auxiliary store.
%
The main overhead is that the auxiliary store records the access counts of each object in the past \XXX{} epochs. 
%
A naive implementation that records the object names~(\eg, block/page addresses, keys of key-value pairs) and \XXX{} counters would incur large overhead.

Following \wtinylfu~\cite{}, \sys minimizes the aforementioned overhead using 
a count-min sketch~\cite{} with a sliding window. 
%
A count-min sketch operates similar to a bloom filter~\cite{} (more specifically, a counting bloom filter~\cite{}), where it is equipped with multiple, say \ABB{H}, hash functions. 
%
A count-min sketch maintains a two-dimensional array where the number of rows equals the number of hash functions~(\ie, \ABB{H}), and the number of columns represents the tradeoff betwen space and accuracy.
%
When an item arrives, it is hashed by each hash function, where each hash function maps the item to a specific column in its corresponding row.
%
The counters of these \ABB{H} cells are incremented. 
%
Upon query, the minimum of the \ABB{H} counters mapped by the hash functions is returned as the estimated count of the item.
%
Therefore, a count-min sketch forgos the need to store each object's name. 

In addition, rather than using a count-min sketch for each epoch, \sys uses only a single sketch. 
%
This is because multiple sketches incur space and performance overhead (since one must query all sketches to obtain the total access counts across all epochs).
%
To ensure that the sketch approximates the access counts in the past \XXX{} epochs, \sys periodically halves all counters in the sketch.  
%
\sys maintains a global counter which got incremented upon each item arrival. 
%
Once the global counter reaches a threshold, \sys halves the global counter and all the counters in the sketch.
%
\wtinylfu~\cite{} shows that this approach is reasonably accurate, supported by both theoretical guarantees on its error bounds and evaluation results. 









% For simplicity, we implement \ABB{K} with a sketch structure with a forget function.
% %
% In the sketch, each object maps a counter with a hash function.
% %
% To simulate the sliding window sketch~\cite{}, \ABB{K} has a global counter and a global indexer for forget function.
% %
% Once inserting an object, \ABB{K} increases the global counter and increases mapped counter for the object.
% %
% When the global counter reaches a threshold~(\eg 8), \ABB{K} halves the counter pointed by the global indexer to forget old records, and then increases the global indexer and resets the global counter for the next counter.
% %
% On average , each insertion only halves $1/threshold$ counters for 2 thresholds' epochs.



% Cite a large number of papers that is implemented with CacheLib; systems 
% reviewers might doubt the simulation.
% Following much prior work~\cite{}, we evaluate \XXX{latency} and \XXX{throughput} in \cachelib~(\S\ref{}) and hit ratio in \libcachesim~(\S\ref{}). 
% %

\PN{Common optimizations.}
%
\sys applies several common performance and space optimizations in prior work. 
%
First, following the existing implementation in \cachelib, \sys uses a hash table to index all cached objects to minimize the lookup time.

Second, following~\cite{}, \sys implements the ghost queue with a hash table. 
%
For each ghost object, \sys stores in the hash table its fingerprint, \hotness counter, and a sequence number, for a total of 8 bytes.
%
\sys uses the sequence number to identify if an object is still in the ghost queue, facilitating lazy object deletion.
%
\sys increments a global sequence number upon each insertion to the ghost queue. 
%
Thus, any object with a sequence number less than the global sequence number minus ghost queue's size has already evicted from the ghost queue.
%
Thanks to the sequence number, \sys lazily removes an evicted ghost object from the hash table only upon its access. 

\PN{Other optimization/implementation.}
%
\sys uses three bits to store the \hotness counter and uses \XXX{} hash for all relevant hash tables/count-min sketch. 
%
\syss current implementation uses a linked list to implement the FIFO queues~(\ie, \XXX{the filter, core, and staging} queues), while one can also implement these queues with ring buffers.  
%

\DZ{Modify the pseudocode to make this part more understandable}
Another optimization in \sys is that it does not delete the object from the ghost queue upon its promotion to the staging queue~(L\XXX{} in \autoref{}).
%
Therefore, \sys does not need to record each object's position in the ghost queue~(L\XXX{}). 
%
This is possible since a stale object in the ghost queue does not affect the correctness of \sys; 
%
\sys only checks if the object is in the ghost queue upon a cache miss~(L\XXX{}), where the object must not be in the staging queue.  

\PN{Tradeoff between cache hit rates and overhead.}
%
Using a count-min sketch for recording objects' popularity and a hash table for the ghost queue may reduce the hit rates of \sys. 
%
In the former case, multiple objects may map to the same counter and the reset operation is an estimation. 
%
In the latter case, \sys may mistakenly view an object in the ghost queue due to fingerprint collisions.
%
However, our evaluation shows that \XXX{}. 



\subsection{Overhead Analysis}
%
\XXX{detailed evaluation in \S\ref{}}


\PN{Computation overhead.} 
%
Upon a cache access, \sys searches the object in the hash table. 
%
A cache hit only requires updating the \hotness counter~(L\XXX{} in \S\ref{}) and the \hotness map, incurring minimal overhead. 
%
On a cache miss, most of the processing time comes from evicting items from the filter or staging queues
%
Both cases involve inserting objects from the filter queue to the main queue~(L\XXX{}), and the latter case requires inserting objects from the staging queue to the main queue~(L\XXX{}).
%
However, in practice, such reinsertions are rare since \XXX{}. 
%
The rest of the operations (\eg, \XXX{}) incur minimal overhead.


\PN{Multicore scalability.}
%
\PN{Update on the hotness: atomic increment.}
\PN{FIFO queues}
\PN{Hash table.}
\PN{Maintaining the metadata of \sys: hotness distribution.}



\PN{Space overhead.} 
%
The space overhead of \sys mainly comes from 1) per-object metadata and 2) the ghost queue; and 3) the auxiliary store.
%
Each metadata is 4 bits, consisting of a \hotness counter~(3 bits) and a 1-bit flag recording if the object is from the ghost queue~(L\XXX{} in \S\ref{}).
%
The ghost queue stores the same amount of objects as the main cache, and each object consumes an additional of 8 bytes. 
%
The count-min sketch in the auxiliary store uses \XXX{} bytes (\XXX{Row, Column, and Count bits}).
%
\XXX{map overhead, epoch counter, globalk counter} 

Consider a typical object size of 4KB, the space overhead of \sys is \XXX{}. 


\subsection{Discussion}
\label{ss:discussion-and-limitations}

\XXX{Simplicity}
\DZ{Argue that while \sys is complicated, it does not affect the efficiency, latency, and throughput of the cache.} 

\XXX{Relationship with respect to S3FIFO}

\PN{Adapting component size.}
%
\sys currently uses fixed sizes for its components.
%
\XXX{Motivate why we need to adapt the sizes of components.}
%

Therefore, to adjust the epoch size and hotness threshold, it is convenient to change the size of \ABB{M} and \ABB{G} and the target portion.
%
However, there is not a proper theoretical model to guide the adjustment of components size with changed types.
%
There are some theoretical models for fixed classification into different components, which generate the MRC of each components and optimize the overall hit ratio~\cite{}.
%
Therefore, we leave the adaption of \sys to future work and use fixed sizes for all the components in this paper.
%

\XXX{If we enlarge the filter cache, the new objects will stay longer in the filter cache.
%
This will increase the performance of a little workloads, in which the new objects are accessed after a period of time.
%
If the filter cache is larger than the time to live of the new objects, the performance will be improved, but we can analyze the workloads and set the filter cache size.
%
The size of the suspicious cache depends on the confidence level of the suspicious objects—if the objects are highly reliable, a larger suspicious cache is more appropriate. 
%
However, in our experiments, we found that the size of this portion of the cache has little impact on overall performance. 
%
When the cache is relatively small, the hit rate is very low, and many strategies can only guess which data is accessed more frequently. 
%
This explains why WTinyLFU exhibits significant performance fluctuations.}



\PN{Adversarial workloads for \sys.}
%
\sys highly relies on epoch and hotness analysis to identify access patterns.
%
(1) Workloads with large reuse which apporaches the epoch size are challenge for epoch analysis.
%
The large reuse would have many hits in \ABB{G} rather than \ABB{M} for the first epoch in \sys, and it will be evicted by suspected data in \ABB{S} at the end of the epoch, leading to many misses.
%
(2) Workloads with greatly changed hotness distribution are challenge for hotness analysis.
%
In this case, the hotness threshold would change frequently, and \ABB{K} takes time to learn it. 
%


Other algorithms also suffer from these workloads, and large cache size or prefetcher can mitigate the influence.
%
Algorithms with a small filter queue~(S3-FIFO~\cite{}, 2Q~\cite{}) also evict data to ghost queue, leading to misses.
%
Algorithms without a sketch~(ARC~\cite{},CACHEUS~\cite{}) also discard useful data due to limited accesses.
%
With large cache size, algorithms can catch marginal accesses.
%
Most of equal hotness counters come from repeated accesses in a sequence, and prefetchers can help to catch them.
%



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/overview.drawio.pdf}
    \caption{Overview for \sys. Cache space consists of five parts: filter cache, main cache, suspicious cache, ghost cache, and CBF. The workflow has three major parts, (1)cache promotion and eviction in four sub-cache. (2)IA-MP objects in the filter cache duels with the tail object in the suspicious cache with CBF. (3)Leverage the metadata in the ghost cache to find FA-FP objects and prefetch sequence accessed objects.\TODO{update}}
    \label{fig:overview}
\end{figure}

\subsection{Properties}
\label{ss:properties}

\syss design achieve the following important properties mentioned in prior cache literature~\cite{}.

% \PN{Cover a wide range of access patterns.}

\PN{Self-adaptive parameters.}
Tuning parameters is notoriously difficult for cache eviction algorithms~\cite{}.

\PN{Lazy promotion and quick demotion.}
%
Recent advances in cache eviction algorithms~\cite{} have shown that lazy promotion and quick demotion can effectively improve cache performance.

\PN{Multicore scalability.}
%
Many Modern cache replacement algorithms are built with cache primitives, such as FIFO queues and LRU~\cite{}.
%
\sys uses FIFO queues to achieve multicore scalability.