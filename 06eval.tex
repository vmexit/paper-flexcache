\section{Evaluation}
\label{s:eval}
Our evaluation answers the following questions:
\squishlist
    \item How does \sys perform compared with state-of-the-art algorithms in various workloads and cache sizes for robustness?
    \item Is \sys efficient in terms of bandwidth and throughput?
    \item Is \sys scalable compared to adaptive algorithms?
\squishend

\DZ{Move all tables under the \\tbl directory}
\DZ{Check how do we draw tables in other SOSP papers in our lab repo.}
\input{06evaltable}

\subsection{Evaluation Setup}

\PN{Environment.}
%
\DZ{Do not mention Kunpeng... }
We evaluate on a \XXX{}-socket machine equipped with \XXX{}-core \XXX{CPU name}.
%
The system runs \XXX{Ubuntu 22.04} and Linux \XXX{kernel 6.12.20}. 
%
We disable hyperthreading, turbo boost to obtain stable results. 

\PN{Baseline.}
%
We compare \sys with \XXX{16} state-of-the-art replacement algorithms, including 1) classical algorithms: \lru, \lfu, and \twoqueue~\cite{}, 
%
2) state-of-the-art static algorithms: \wtinylfu~\cite{}, \sthreefifo~\cite{}, \lirs~\cite{}, \XXX{} 
and 3) state-of-the-art adaptive algorithms: \arc~\cite{}, \car~\cite{}, \cacheus~\cite{}, \lecar~\cite{}. 
%
Note that \XXX{} are based on machine learning. 
%

We used the implementations from \XXX{\libcachesim~\cite{libcachesim}} for these baseline algorithms. 
%
\XXX{We have checked that the implementations conform to the original papers, and used the default parameters from the original papers.}
%

We compare \sys with all the baseline algorithms for hit ratio. 
%
For the \XXX{}, we select \XXX{representative algorithms from each category}



\PN{Workload.} 
%
\DZ{Consider adding more datasets}
%
We used \XXX{11} open-source datasets with \XXX{} traces to evaluate \sys, as shown in~\autoref{tab:traces}.
%
These traces are diverse in both application domains, including key-value, object CDN, and block caches, and time periods, spanning from 2008 to 2023.
%
In total, these datasets contain \XXX{} requests, \XXX{} objects, \XXX{} TB of traffic for a total of \XXX{} TB of data.
%
To facilitate analysis, following prior work~\cite{}, we split Tencent 
CBS~\cite{zhang2018tencent} and Alibaba~\cite{Alibaba} into per-tenant traces.
%

\PN{Methodology.}
%
\DZ{Cite as many prior work as possible to justify our methodology. Systems folks are very unhappy about simulation.}
%
Due to a large number of traces, following the conventional evaluation approach in prior work~\cite{}, we used a high-performance simulator \libcachesim~\cite{}
to evaluate hit ratio. 
%
In total, we ran \XXX{} requests, using \XXX{a million CPU hours}.

We report the hit ratio using two metrics. 
%
\XXX{Need to motivate why doing so...}
The first one is the hit ratio improvement over \lru~(\ie, $\frac{HR_{algo} - HR_{lru}}{HR_{lru}}$), a metric widely used in prior work~\cite{}.
%
The second one is the relative hit ratio compared to the dominant algorithm(\ie, $\frac{HR_{algo}}{HR_{dominant}}$), where the dominant algorithm is the one with the highest hit ratio. 
%
This metric reflects how well an algorithm generalizes across different workloads.
%



\XXX{For datasets with more than 100 traces, we report both the distribution of relative hit rates and tail performance. 
%
For those with fewer than 100 traces, we only calculate the average performance.}
%


\begin{figure*}[t]
    \centering
    \input{data/hitimp}
    \caption{Relative hit ratio improvements based on LRU.}
    \label{fig:hitimp}
\end{figure*}

\begin{figure*}[t]
    \centering
    \input{data/hitimpall}
    \caption{Relative hit ratio improvements based on LRU.}
    \label{fig:hitimpall}
\end{figure*}

\subsection{Hit Ratio}
%
For each trace, we evaluate the hit ratio with cache sizes ranging from \XXX{0.3\%} to \XXX{20\%} of the working set size~(\ie, the total number of objects in the trace) 
%
Following prior work~\cite{}, we omit the results if the absolute cache size is less than 100 objects, since the size is too small to be practical, 







\textbf{Average relative hit ratio.} We evaluate \sys and SOTA policies with benchmarks in table~\ref{tab:traces} in different cache size. 
%
The SOTA policies are widely used in production systems or compared in prior work.
%
To demonstrate the adaptability of different replacement policies:  
%
(1) We select a diverse set of traces, including block, object, and key-value traces;
%
(2) We conduct experiments using six different cache sizes relative to the WSS: 0.3\%, 1\%, 3\%, 10\%, 20\%, and 40\%;  
%
(3) We identify the highest hit ratio among SOTA strategies as the potential hit ratio and compare each policies' relative hit ratio to it.
%
Figure~\ref{fig:averagerhr} shows the average relative hit ratio of \sys and SOTA policies in benchmarks with less than 100 traces.
%
%Figure~\ref{fig:tailhrh} shows the average and tail performance of \sys and SOTA policies in benchmarks with more than 100 traces.
%
\sys outperforms the SOTA policies in most benchmarks, achieving either the best performance or at least 99\% of the optimal potential hit ratio across most tests and cache size configurations.
%
While others exhibit about 10\% to 40\% performance degradation.

\textbf{Tail performance.}The P1 tail performance represents the upper bound of the lowest 1\% relative hit ratio of the traces in the benchmarks.
%
For benchmarks with more than 100 workloads, we evaluate the distribution of relative hit rates across different cache sizes.  
%
As shown in figure~\ref{fig:tailhrh}, we present the data distribution for three benchmarks under three different cache sizes—the larger the shape, the larger the cache size. 
%
Tail performance above the P10 is relatively close, so we present their average values.
%
When the cache size is larger, the tail performance is better for amlost all policies.
%
\sys achieves P1 tail performance of 80\% traces and P5 tail performance of 95\% traces, while the SOTA policies' P1 tail performance ranges from 10\% to 60\% and P5 tail performance ranges from 30\% to 80\%.

\textbf{Imporvements over FIFO.} FIFO is a basic policy, so prior studies often compare performance gains over FIFO. 
%
We also tested the performance improvements of different replacement policies, as well as our current scheme with some modules disabled, relative to FIFO.
%
As shown in figure~\ref{fig:hitimp}, \sys disable the CBF with \sys-C and disable the prefetch with \sys-P.
%
Benchmarks of type \cc{object} and \cc{kv} show relatively small performance improvements, mainly due to their more random access patterns—most policies differ by less than 0.5\%.
%
In contrast, benchmarks based on \cc{block}-level access exhibit significantly larger improvements, with some strategies achieving up to a 57\% performance gain compared to FIFO.

\textbf{Byte miss ratio.}\TODO{}

\textbf{WTinyLFU}~\cite{} uses a small LRU and a part of SLRU to filter IA-FP objects, a part of SLRU for FA-MP objects, and a CBF to find suspicious IA-MP objects.
%
As discussed by prior work~\cite{}, its LRU filter is too small to hold FA-FP objects.
%
To filter the interference of recency-based objects to the frequency-based objects, it does not consider the access frequency of objects in the LRU cache, so it is conservative to hold FA objects, which discards FA-MP objects for many times.
%
What's more, the CBF for IA-MP objects is not accurate as the ghost cache, so it suffers from a large performance degradation even with a large cache size in figure~\ref{fig:averagerhr}(b) and (f).

\textbf{TwoQ}~\cite{} has a FIFO queue for the filter cache. a LRU queue for the main cache, and a ghost cache.
%
TwoQ uses the ghost cache to discover the objects accessed with the second chance, but it evicts all the objects from the FIFO queue to the ghost cache, and then promotes the objects from the ghost cache to the main cache.
%
All the objects in the main cache will encounter a miss in the ghost cache, this will cause the performance degradation.
%
The design is for the specific workloads, and it is not suitable for the workloads with many FA-FP objects as pollution and 

\textbf{ARC}~\cite{} uses two parts of caches for recency-based and frequency-based objects and two ghost caches for each part.
%
Hits in the recency-based cache promote the objects to the frequency-based cache.
%
It evicts the objects from the two main caches to the corresponding ghost caches, and each hit in the ghost caches is a duel for the main cache space.
%
This adaption is effective in some cases, like figure~\ref{fig:averagerhr}(d) with a really small cache size (0.3\% WSS).
%
In other cases, it fails to work with phase-aware as discussed in ~(\autoref{ss:dynamic-adjust}).

\textbf{LeCaR and Cacheus}~\cite{} leverages two basic policies for dueling and two ghost cache for evicted objects.
%
They implement the two policies with probability and evict the objects to the corresponding ghost cache.
%
Once hit in a ghost cache, they increase the probability of the corresponding policy.
%
The performance is highly sensitive to the basic policies and the probability transection function.
%
What's more, they only consider the influence of hits in ghost caches, but ignore the hits in the main caches.

\textbf{LIRS}~\cite{} has filter, main and ghost caches, but it limits the space of the ghost cache by IRR.
%
It evicts objects with access timestamp in the ghost cache lower than the lower bound of the access timestamp in the main cache.
%
It is a conservative way to hold suspicious objects in some cases, extremely useful in FIU as shown in figure~\ref{fig:averagerhr}(a).
%
We find that there are many repeated access in FIU, so LIRS avoids the pollution of long repeated access objects.
%
However, it is too conservative in other cases, like figure~\ref{fig:averagerhr}(b) MetaCDN and (f) TencentPhoto, which leads to a performance degradation.
%
\sys integrates prefetching for the repeated access objects, otherwise, we need more effort to tag these objects.
%
If we disable the prefetching, \sys-P is better than LIRS in MSR but worse in FIU.

\textbf{S3FIFO}~\cite{} has the most similiar design to \sys. It has three FIFO queues for the filter, main and ghost caches.
%
It uses a fixed criteria to promote the objects from the filter and ghost caches to the main cache (e.g., 3 for objects in filter cache and 1 for objects in ghost cache).
%
S3FIFO does not consider the access frequency distribution of the phase, so it ignores the affection of the suspicious objects, both IA-MP in filter cache and FA-MP in ghost cache.
%
S3FIFO achieves a good performance in CDN and KVs benchmarks, but it lacks the adaptability for some access patterns, so its tail performance is poor. 


\textbf{Weighted policies} includes LHD~\cite{}, Hyperbolic~\cite{}, and GDSF~\cite{}.
%
They leverage a metric to calculate the weight of the objects, and evict the objects with the lowest weight.
%
The performance of these policies is sensitive to the access distribution of the workloads and its weight function.
%
As shown in figure~\ref{fig:tailhrh}, Hyperbolic and GDSF both suffer from a performance degradation in the tail performance, while LHD is better than GDSF and Hyperbolic.
%
 

\textbf{Adversarial workloads for \sys.} We can construct some adversarial access patterns on which \sys performs poorly, and only a customized policy performs better.
%
One is the challenge for the second access.
%
For workloads which access frequency follows the zipfan low, there are many objects accessed a few times.
%
Therefore, the second access of a object will fall in the ghost cache, and it is hard to decide whether to put it in the main cache, which causes the requests miss and cache pollution.
%
It is adversarial for all the policies which partitions the cache space, for example, LIRS, 2Q and S3FIFO.
%
Another is the challenge for access changes for multiple groups of objects.
%
If a group of objects with high access frequency resident in the main cache, and another group of objects appears in the filter cache with some access, it is hard to decide whether to evict the objects in the main cache.
%
We will not know which group of objects will appear in the future, so there are some misses during the access change.
%
It is adversarial for other policies, e.g., Cacheus, ARC.
%
\sys keep some suspicious objects like the second access objects and use the access frequency distribution to hold part of group objects in the main cache.
%
This is helpful for the tail performance, but it is not the best performance in some cases.

\begin{figure}[t]
    \centering
    \input{data/rhrcdf}
    \caption{\TODO{}}
    \label{fig:rhrcdf}
\end{figure}

\subsection{Breakdown performance}

\paragraph{\TODO{Sensitive Analysis}.}
%
We evaluated the impact of different modules on \sys. 
%
In the experiments, we individually disabled the prefetching and CBF modules to analyze their respective contributions to performance.
%
Additionally, we adjusted the size of the staging space to determine how different the filter and staging queue size affect overall performance.

\textbf{CBF}
%
We disable the CBF and show the performance of \sys-C in figure~\ref{fig:break}.
%
When the cache size is small, the CBF contributes a lot for the tail performance, without which the 10\% of the traces will degrade serverly.
%
When the cache size is large, it affect about 8\% of the traces.
%
In the worst case, \sys-C achieves 42\% of the relative hit ratio, and \sys achieve 68\% of the relative hit ratio for small cache size.
%
When we disable the CBF, for \sys-C outperform the \sys in 3\% to 10\% of the traces for small cache size, and this phenomenon is not evident when the cache size is large.
%
Overall, CBF contributes about 3\% for tail performance.

The CBF is a key for IA-MP objects in \sys. 
%
When the cache size is small, as discussed in \autoref{ss:dynamic-adjust}, many reaccesses of the objects are out of the scope of the ghost cache, so we need to record the access information in the CBF
%
Many objects in the main cache are accessed with low frequency, so we can put some of suspicious objects in the suspicious cache.
%
In this case, WTinyLFU is effective and the CBF selects the objects with high access frequency to put in the main cache.
\TODO{CBF query times}
\textbf{Prefetching}
%
We disable the prefetching and show the performance of \sys-P in figure~\ref{fig:break}.
%
Prefetching has limited impact on CDN and KVs workloads, because there is a small part of objects accessed in sequence, \XXX{even if with high prefetching hit ratio}.
%
For block workloads, prefetching contributes about 5\% traces in Alibaba and 20\% traces in Cloudphysics with small cache size.
%
For larger caches, the impact of prefetching is broader and more pronounced than the CBF in improving performance, for about 40\% of traces in Cloudphysics.
%
Without prefetching, the performance of \sys-P is about 95\% of \sys for small cache size and 98\% for large cache size.
%
With large cache size, \sys has more space to hold the objects for perfetching, so prefetching contributes about 5\% for the tail performance with large cache size.

\TODO{Prefetch hit ratio}

\textbf{Suspicious cache size.}

\textbf{Ghost cache size.}

\subsection{Throughput}


To enable trace reading, we referenced the previous code~\cite{yang2023fifo}, allowing it to replay traces in a closed loop for benchmarking. 
%
Considering the significant backend latency caused by cache misses in actual production, we added a latency penalty to each cache miss to ensure it reflects real-world production conditions. 
%
We evaluated its throughput under different workloads and cache sizes.



