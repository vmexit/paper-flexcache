\section{Evaluation}
\label{s:eval}
Our evaluation answers the following questions:
\squishlist
    \item How does \sys perform compared with SOTA algorithms in various workloads and cache sizes for robustness?
    \item Is \sys efficient in terms of bandwidth and throughput?
    \item Is \sys scalable compared to adaptive algorithms?
\squishend

\input{06evaltable}
\subsection{Evaluation setup}
\textbf{Traces.} We evaluated \sys with 4853 production traces from 11 datasets. 
%
These traces cover key-value, CDN object, and block caches, spanning from \XXX{date}.
%
\TODO{In total, the datasets contain 856 billion requests to 61 billion objects, 21,088 TB traffic for total 3,753 TB of data, as shown in table~\ref{tab:traces}.}
%
In many large-scale distributed caching system, traces in workloads are served by multiple servers for multi-tenanted services.
%
Therefore, we split the Tencent CBS~\cite{} and Alibaba~\cite{} benchmarks into per-tenant traces as prior work~\cite{}.


\textbf{Simulator.} We implement \sys in libCacheSim~\cite{} as prior work.
%
For the state-of-the-art algorithms~\cite{}, we use the original implementations of the algorithms with the same parameters.
%
%We integrate the cache-size-based phase partitioning method and data reuse time distribution analysis into the analysis module of the simulator.
%
We do not consider the metadata size of different algorithms, for \sys requires limited metadata and production systems usually have more detailed metadata.
%
In this scope, we ignore the data management as prior work~\cite{} for benchmarks with different object sizes.
%
The production systems store objects of similar sizes in the same slab class, which introduces fragmentation but simplifies memory management.
%
It is non-trivial to consider the object size in a system without a slab-based memory management.

\textbf{Metrics and methodology.} 
%
We evaluate different algorithms across various cache sizes for robustness and efficiency.
%
Cache sizes range from 0.3\% to 20\% of the working set size (WSS), defined as the total number of objects in the trace.
%
In some cases, the 0.3\% WSS is too small to be practical, whose cache size is less than 1000 objects, so we ignore it.
%
With the help of slabs, we ignore the object size for ideal case as prior work~\cite{}.
%
To evaluate the efficiency of cache utility, we also consider the objects and consider the byte hit ratio.

To exhibit the robustness, we evaluate 16 state-of-the-art algorithms and compare them with with a dominant algorithm, which achieves the highest hit ratio for the trace with a specific cache size.
%
Compared with the dominant algorithm, each algorithm have a relative hit ratio distribution for benchmarks in different cache sizes.
%
Additionally, we also calculate the performance improvement relative to LRU.
%
For benchmarks with more than 100 traces, we report both the distribution of relative hit rates and the tail performance. 
%
For those with fewer than 100 traces, we only calculate the average performance.

\TODO{sensitive analysis}
We evaluate the impact of different modules on the \sys. 
%
In the experiments, we individually disable the prefetching and CBF modules to analyze their respective contributions to performance. 
%
Additionally, we adjust the size of the suspicious space to observe how different cache sizes of filter and suspicious cache affect overall performance.


\begin{figure*}[t]
    \centering
    \input{data/hitimp}
    \caption{Relative hit ratio improvements based on FIFO. We disable the CBF with \sys-C and disable the prefetch with \sys-P.}
    \label{fig:hitimp}
\end{figure*}
\subsection{Performance for different cache sizes}
\textbf{Average relative hit ratio.} We evaluate \sys and SOTA policies with benchmarks in table~\ref{tab:traces} in different cache size. 
%
The SOTA policies are widely used in production systems or compared in prior work.
%
To demonstrate the adaptability of different replacement policies:  
%
(1) We select a diverse set of traces, including block, object, and key-value traces;
%
(2) We conduct experiments using six different cache sizes relative to the WSS: 0.3\%, 1\%, 3\%, 10\%, 20\%, and 40\%;  
%
(3) We identify the highest hit ratio among SOTA strategies as the potential hit ratio and compare each policies' relative hit ratio to it.
%
Figure~\ref{fig:averagerhr} shows the average relative hit ratio of \sys and SOTA policies in benchmarks with less than 100 traces.
%
%Figure~\ref{fig:tailhrh} shows the average and tail performance of \sys and SOTA policies in benchmarks with more than 100 traces.
%
\sys outperforms the SOTA policies in most benchmarks, achieving either the best performance or at least 99\% of the optimal potential hit ratio across most tests and cache size configurations.
%
While others exhibit about 10\% to 40\% performance degradation.

\textbf{Tail performance.}The P1 tail performance represents the upper bound of the lowest 1\% relative hit ratio of the traces in the benchmarks.
%
For benchmarks with more than 100 workloads, we evaluate the distribution of relative hit rates across different cache sizes.  
%
As shown in figure~\ref{fig:tailhrh}, we present the data distribution for three benchmarks under three different cache sizes—the larger the shape, the larger the cache size. 
%
Tail performance above the P10 is relatively close, so we present their average values.
%
When the cache size is larger, the tail performance is better for amlost all policies.
%
\sys achieves P1 tail performance of 80\% traces and P5 tail performance of 95\% traces, while the SOTA policies' P1 tail performance ranges from 10\% to 60\% and P5 tail performance ranges from 30\% to 80\%.

\textbf{Imporvements over FIFO.} FIFO is a basic policy, so prior studies often compare performance gains over FIFO. 
%
We also tested the performance improvements of different replacement policies, as well as our current scheme with some modules disabled, relative to FIFO.
%
As shown in figure~\ref{fig:hitimp}, \sys disable the CBF with \sys-C and disable the prefetch with \sys-P.
%
Benchmarks of type \cc{object} and \cc{kv} show relatively small performance improvements, mainly due to their more random access patterns—most policies differ by less than 0.5\%.
%
In contrast, benchmarks based on \cc{block}-level access exhibit significantly larger improvements, with some strategies achieving up to a 57\% performance gain compared to FIFO.

\textbf{Byte miss ratio.}\TODO{}

\textbf{WTinyLFU}~\cite{} uses a small LRU and a part of SLRU to filter IA-FP objects, a part of SLRU for FA-MP objects, and a CBF to find suspicious IA-MP objects.
%
As discussed by prior work~\cite{}, its LRU filter is too small to hold FA-FP objects.
%
To filter the interference of recency-based objects to the frequency-based objects, it does not consider the access frequency of objects in the LRU cache, so it is conservative to hold FA objects, which discards FA-MP objects for many times.
%
What's more, the CBF for IA-MP objects is not accurate as the ghost cache, so it suffers from a large performance degradation even with a large cache size in figure~\ref{fig:averagerhr}(b) and (f).

\textbf{TwoQ}~\cite{} has a FIFO queue for the filter cache. a LRU queue for the main cache, and a ghost cache.
%
TwoQ uses the ghost cache to discover the objects accessed with the second chance, but it evicts all the objects from the FIFO queue to the ghost cache, and then promotes the objects from the ghost cache to the main cache.
%
All the objects in the main cache will encounter a miss in the ghost cache, this will cause the performance degradation.
%
The design is for the specific workloads, and it is not suitable for the workloads with many FA-FP objects as pollution and 

\textbf{ARC}~\cite{} uses two parts of caches for recency-based and frequency-based objects and two ghost caches for each part.
%
Hits in the recency-based cache promote the objects to the frequency-based cache.
%
It evicts the objects from the two main caches to the corresponding ghost caches, and each hit in the ghost caches is a duel for the main cache space.
%
This adaption is effective in some cases, like figure~\ref{fig:averagerhr}(d) with a really small cache size (0.3\% WSS).
%
In other cases, it fails to work with phase-aware as discussed in ~(\autoref{ss:dynamic-adjust}).

\textbf{LeCaR and Cacheus}~\cite{} leverages two basic policies for dueling and two ghost cache for evicted objects.
%
They implement the two policies with probability and evict the objects to the corresponding ghost cache.
%
Once hit in a ghost cache, they increase the probability of the corresponding policy.
%
The performance is highly sensitive to the basic policies and the probability transection function.
%
What's more, they only consider the influence of hits in ghost caches, but ignore the hits in the main caches.

\textbf{LIRS}~\cite{} has filter, main and ghost caches, but it limits the space of the ghost cache by IRR.
%
It evicts objects with access timestamp in the ghost cache lower than the lower bound of the access timestamp in the main cache.
%
It is a conservative way to hold suspicious objects in some cases, extremely useful in FIU as shown in figure~\ref{fig:averagerhr}(a).
%
We find that there are many repeated access in FIU, so LIRS avoids the pollution of long repeated access objects.
%
However, it is too conservative in other cases, like figure~\ref{fig:averagerhr}(b) MetaCDN and (f) TencentPhoto, which leads to a performance degradation.
%
\sys integrates prefetching for the repeated access objects, otherwise, we need more effort to tag these objects.
%
If we disable the prefetching, \sys-P is better than LIRS in MSR but worse in FIU.

\textbf{S3FIFO}~\cite{} has the most similiar design to \sys. It has three FIFO queues for the filter, main and ghost caches.
%
It uses a fixed criteria to promote the objects from the filter and ghost caches to the main cache (e.g., 3 for objects in filter cache and 1 for objects in ghost cache).
%
S3FIFO does not consider the access frequency distribution of the phase, so it ignores the affection of the suspicious objects, both IA-MP in filter cache and FA-MP in ghost cache.
%
S3FIFO achieves a good performance in CDN and KVs benchmarks, but it lacks the adaptability for some access patterns, so its tail performance is poor. 


\textbf{Weighted policies} includes LHD~\cite{}, Hyperbolic~\cite{}, and GDSF~\cite{}.
%
They leverage a metric to calculate the weight of the objects, and evict the objects with the lowest weight.
%
The performance of these policies is sensitive to the access distribution of the workloads and its weight function.
%
As shown in figure~\ref{fig:tailhrh}, Hyperbolic and GDSF both suffer from a performance degradation in the tail performance, while LHD is better than GDSF and Hyperbolic.
%
 

\textbf{Adversarial workloads for \sys.} We can construct some adversarial access patterns on which \sys performs poorly, and only a customized policy performs better.
%
One is the challenge for the second access.
%
For workloads which access frequency follows the zipfan low, there are many objects accessed a few times.
%
Therefore, the second access of a object will fall in the ghost cache, and it is hard to decide whether to put it in the main cache, which causes the requests miss and cache pollution.
%
It is adversarial for all the policies which partitions the cache space, for example, LIRS, 2Q and S3FIFO.
%
Another is the challenge for access changes for multiple groups of objects.
%
If a group of objects with high access frequency resident in the main cache, and another group of objects appears in the filter cache with some access, it is hard to decide whether to evict the objects in the main cache.
%
We will not know which group of objects will appear in the future, so there are some misses during the access change.
%
It is adversarial for other policies, e.g., Cacheus, ARC.
%
\sys keep some suspicious objects like the second access objects and use the access frequency distribution to hold part of group objects in the main cache.
%
This is helpful for the tail performance, but it is not the best performance in some cases.

\begin{figure}[t]
    \centering
    \input{data/break}
    \caption{Relative hit raitio improvements based on \sys. We disable the CBF with \sys-C and disable the prefetch with \sys-P.}
    \label{fig:break}
\end{figure}

\subsection{Breakdown performance}
\textbf{CBF}
%
We disable the CBF and show the performance of \sys-C in figure~\ref{fig:break}.
%
When the cache size is small, the CBF contributes a lot for the tail performance, without which the 10\% of the traces will degrade serverly.
%
When the cache size is large, it affect about 8\% of the traces.
%
In the worst case, \sys-C achieves 42\% of the relative hit ratio, and \sys achieve 68\% of the relative hit ratio for small cache size.
%
When we disable the CBF, for \sys-C outperform the \sys in 3\% to 10\% of the traces for small cache size, and this phenomenon is not evident when the cache size is large.
%
Overall, CBF contributes about 3\% for tail performance.

The CBF is a key for IA-MP objects in \sys. 
%
When the cache size is small, as discussed in \autoref{ss:dynamic-adjust}, many reaccesses of the objects are out of the scope of the ghost cache, so we need to record the access information in the CBF
%
Many objects in the main cache are accessed with low frequency, so we can put some of suspicious objects in the suspicious cache.
%
In this case, WTinyLFU is effective and the CBF selects the objects with high access frequency to put in the main cache.
\TODO{CBF query times}
\textbf{Prefetching}
%
We disable the prefetching and show the performance of \sys-P in figure~\ref{fig:break}.
%
Prefetching has limited impact on CDN and KVs workloads, because there is a small part of objects accessed in sequence, \XXX{even if with high prefetching hit ratio}.
%
For block workloads, prefetching contributes about 5\% traces in Alibaba and 20\% traces in Cloudphysics with small cache size.
%
For larger caches, the impact of prefetching is broader and more pronounced than the CBF in improving performance, for about 40\% of traces in Cloudphysics.
%
Without prefetching, the performance of \sys-P is about 95\% of \sys for small cache size and 98\% for large cache size.
%
With large cache size, \sys has more space to hold the objects for perfetching, so prefetching contributes about 5\% for the tail performance with large cache size.

\TODO{Prefetch hit ratio}

\textbf{Suspicious cache size.}

\textbf{Ghost cache size.}

\subsection{Throughput}
\TODO{}



